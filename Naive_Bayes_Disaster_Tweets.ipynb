{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df481245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\teba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\teba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell to import nltk\n",
    "import nltk\n",
    "from os import getcwd\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d806ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e930262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a10df",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31969e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(text):\n",
    "    import re\n",
    "    import string\n",
    "    # Convert text to lowercase\n",
    "    outText = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    outText = re.sub(r'\\d+', '', outText)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    outText =  outText.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    \n",
    "    #Remove whitespaces\n",
    "    outText = outText.strip()\n",
    "    \n",
    "    #Remove stopwords\n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tokens = word_tokenize(outText)\n",
    "    outText = [i for i in tokens if not i in stop_words]\n",
    "    \n",
    "    #Lemmatization\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    result=[]\n",
    "    for word in outText:\n",
    "        result.append(lemmatizer.lemmatize(word))\n",
    "        \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde7b6a",
   "metadata": {},
   "source": [
    "### Read the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520f230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"train.csv\")  \n",
    "test_dataset = pd.read_csv(\"test.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "795c93f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e1746c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c4ae1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_dataset[[\"text\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ee3e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa918eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_dataset[[\"text\"]].values\n",
    "y = train_dataset[[\"target\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4990e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'],\n",
       "       ['Forest fire near La Ronge Sask. Canada'],\n",
       "       [\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"],\n",
       "       ...,\n",
       "       ['M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ'],\n",
       "       ['Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.'],\n",
       "       ['The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265518e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f70bed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest fire near La Ronge Sask. Canada\n"
     ]
    }
   ],
   "source": [
    "print((X[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb9054cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a sentence: \n",
      " Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['deed', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'u']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a sentence: \\n', X[0][0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', preProcessing(X[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c99e3",
   "metadata": {},
   "source": [
    "## Part 1.1 Implementing your helper functions\n",
    "\n",
    "To help you train your naive bayes model, you will need to compute a dictionary where the keys are a tuple (word, label) and the values are the corresponding frequency.  Note that the labels we'll use here are 1 for positive and 0 for negative.\n",
    "\n",
    "You will also implement a lookup helper function that takes in the `freqs` dictionary, a word, and a label (1 or 0) and returns the number of times that word and label tuple appears in the collection of tweets.\n",
    "\n",
    "For example: given a list of tweets `[\"i am rather excited\", \"you are rather happy\"]` and the label 1, the function will return a dictionary that contains the following key-value pairs:\n",
    "\n",
    "{\n",
    "    (\"rather\", 1): 2,\n",
    "    (\"happi\", 1) : 1, \n",
    "    (\"excit\", 1) : 1\n",
    "}\n",
    "\n",
    "- Notice how for each word in the given string, the same label 1 is assigned to each word.\n",
    "- Notice how the words \"i\" and \"am\" are not saved, since it was removed by process_tweet because it is a stopword.\n",
    "- Notice how the word \"rather\" appears twice in the list of tweets, and so its count value is 2.\n",
    "\n",
    "#### Instructions\n",
    "Create a function `count_tweets` that takes a list of tweets as input, cleans all of them, and returns a dictionary.\n",
    "- The key in the dictionary is a tuple containing the stemmed word and its class label, e.g. (\"happi\",1).\n",
    "- The value the number of times this word appears in the given collection of tweets (an integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "858680c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sentence(result, tweets, ys):\n",
    "    '''\n",
    "    Input:\n",
    "        result: a dictionary that will be used to map each pair to its frequency\n",
    "        tweets: a list of tweets\n",
    "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
    "    Output:\n",
    "        result: a dictionary mapping each pair to its frequency\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in preProcessing(tweet):\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word , y)\n",
    "            \n",
    "            # if the key exists in the dictionary, increment the count\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "\n",
    "            # else, if the key is new, add it to the dictionary and set the count to 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aa5c3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('happy', 1): 1, ('tricked', 0): 1, ('sad', 0): 1, ('tired', 0): 2}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing your function\n",
    "\n",
    "result = {}\n",
    "sentence = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
    "ys = [1, 0, 0, 0, 0]\n",
    "count_sentence(result, sentence, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f54a9e",
   "metadata": {},
   "source": [
    "# Part 2: Train your model using Naive Bayes\n",
    "\n",
    "Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n",
    "\n",
    "#### So how do you train a Naive Bayes classifier?\n",
    "- The first part of training a naive bayes classifier is to identify the number of classes that you have.\n",
    "- You will create a probability for each class.\n",
    "$P(D_{pos})$ is the probability that the document is positive.\n",
    "$P(D_{neg})$ is the probability that the document is negative.\n",
    "Use the formulas as follows and store the values in a dictionary:\n",
    "\n",
    "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
    "\n",
    "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
    "\n",
    "Where $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1348fa3",
   "metadata": {},
   "source": [
    "#### Prior and Logprior\n",
    "\n",
    "The prior probability represents the underlying probability in the target population that a tweet is positive versus negative.  In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n",
    "\n",
    "The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
    "We can take the log of the prior to rescale it, and we'll call this the logprior\n",
    "\n",
    "$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$.\n",
    "\n",
    "Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$.  So the logprior can also be calculated as the difference between two logs:\n",
    "\n",
    "$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f5b035",
   "metadata": {},
   "source": [
    "#### Log likelihood\n",
    "To compute the loglikelihood of that very same word, we can implement the following equations:\n",
    "\n",
    "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2e034",
   "metadata": {},
   "source": [
    "##### Create `freqs` dictionary\n",
    "- Given your `count_tweets` function, you can compute a dictionary called `freqs` that contains all the frequencies.\n",
    "- In this `freqs` dictionary, the key is the tuple (word, label)\n",
    "- The value is the number of times it has appeared.\n",
    "\n",
    "We will use this dictionary in several parts of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f5a187c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41977c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_2d_array_to_list(x):\n",
    "    new_X = []\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        new_X.append(x[i][0])\n",
    "        \n",
    "        \n",
    "    return new_X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac72d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = []\n",
    "new_y = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    new_X.append(X[i][0])\n",
    "    new_y.append(y[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7165175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb31da56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f26a517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the freqs dictionary for later uses\n",
    "freqs = count_sentence({}, new_X, new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "163184af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('deed', 1): 1,\n",
       " ('reason', 1): 8,\n",
       " ('earthquake', 1): 47,\n",
       " ('may', 1): 50,\n",
       " ('allah', 1): 6,\n",
       " ('forgive', 1): 1,\n",
       " ('u', 1): 107,\n",
       " ('forest', 1): 50,\n",
       " ('fire', 1): 262,\n",
       " ('near', 1): 47,\n",
       " ('la', 1): 24,\n",
       " ('ronge', 1): 1,\n",
       " ('sask', 1): 1,\n",
       " ('canada', 1): 10,\n",
       " ('resident', 1): 8,\n",
       " ('asked', 1): 1,\n",
       " ('shelter', 1): 6,\n",
       " ('place', 1): 16,\n",
       " ('notified', 1): 1,\n",
       " ('officer', 1): 31,\n",
       " ('evacuation', 1): 42,\n",
       " ('order', 1): 21,\n",
       " ('expected', 1): 11,\n",
       " ('people', 1): 106,\n",
       " ('receive', 1): 2,\n",
       " ('wildfire', 1): 80,\n",
       " ('california', 1): 115,\n",
       " ('got', 1): 32,\n",
       " ('sent', 1): 4,\n",
       " ('photo', 1): 28,\n",
       " ('ruby', 1): 1,\n",
       " ('alaska', 1): 5,\n",
       " ('smoke', 1): 12,\n",
       " ('pours', 1): 1,\n",
       " ('school', 1): 32,\n",
       " ('rockyfire', 1): 4,\n",
       " ('update', 1): 33,\n",
       " ('hwy', 1): 10,\n",
       " ('closed', 1): 17,\n",
       " ('direction', 1): 4,\n",
       " ('due', 1): 24,\n",
       " ('lake', 1): 9,\n",
       " ('county', 1): 31,\n",
       " ('cafire', 1): 2,\n",
       " ('flood', 1): 79,\n",
       " ('disaster', 1): 118,\n",
       " ('heavy', 1): 18,\n",
       " ('rain', 1): 36,\n",
       " ('cause', 1): 31,\n",
       " ('flash', 1): 17,\n",
       " ('flooding', 1): 37,\n",
       " ('street', 1): 24,\n",
       " ('manitou', 1): 1,\n",
       " ('colorado', 1): 10,\n",
       " ('spring', 1): 12,\n",
       " ('area', 1): 42,\n",
       " ('im', 1): 56,\n",
       " ('top', 1): 16,\n",
       " ('hill', 1): 9,\n",
       " ('see', 1): 28,\n",
       " ('wood', 1): 3,\n",
       " ('there', 1): 17,\n",
       " ('emergency', 1): 76,\n",
       " ('happening', 1): 8,\n",
       " ('building', 1): 94,\n",
       " ('across', 1): 13,\n",
       " ('afraid', 1): 3,\n",
       " ('tornado', 1): 27,\n",
       " ('coming', 1): 16,\n",
       " ('three', 1): 23,\n",
       " ('died', 1): 21,\n",
       " ('heat', 1): 31,\n",
       " ('wave', 1): 30,\n",
       " ('far', 1): 16,\n",
       " ('haha', 1): 4,\n",
       " ('south', 1): 22,\n",
       " ('tampa', 1): 3,\n",
       " ('getting', 1): 18,\n",
       " ('flooded', 1): 4,\n",
       " ('hah', 1): 1,\n",
       " ('wait', 1): 6,\n",
       " ('second', 1): 18,\n",
       " ('live', 1): 30,\n",
       " ('gon', 1): 11,\n",
       " ('na', 1): 17,\n",
       " ('fvck', 1): 1,\n",
       " ('raining', 1): 1,\n",
       " ('florida', 1): 4,\n",
       " ('tampabay', 1): 1,\n",
       " ('day', 1): 45,\n",
       " ('ive', 1): 7,\n",
       " ('lost', 1): 14,\n",
       " ('count', 1): 2,\n",
       " ('bago', 1): 2,\n",
       " ('myanmar', 1): 19,\n",
       " ('arrived', 1): 2,\n",
       " ('damage', 1): 34,\n",
       " ('bus', 1): 37,\n",
       " ('multi', 1): 1,\n",
       " ('car', 1): 73,\n",
       " ('crash', 1): 89,\n",
       " ('breaking', 1): 25,\n",
       " ('whats', 0): 22,\n",
       " ('man', 0): 63,\n",
       " ('love', 0): 94,\n",
       " ('fruit', 0): 6,\n",
       " ('summer', 0): 28,\n",
       " ('lovely', 0): 7,\n",
       " ('car', 0): 39,\n",
       " ('fast', 0): 15,\n",
       " ('goooooooaaaaaal', 0): 1,\n",
       " ('ridiculous', 0): 3,\n",
       " ('london', 0): 9,\n",
       " ('cool', 0): 22,\n",
       " ('skiing', 0): 1,\n",
       " ('wonderful', 0): 5,\n",
       " ('day', 0): 97,\n",
       " ('looooool', 0): 1,\n",
       " ('wayi', 0): 1,\n",
       " ('cant', 0): 81,\n",
       " ('eat', 0): 6,\n",
       " ('shit', 0): 37,\n",
       " ('nyc', 0): 4,\n",
       " ('last', 0): 56,\n",
       " ('week', 0): 32,\n",
       " ('girlfriend', 0): 6,\n",
       " ('cooool', 0): 1,\n",
       " ('like', 0): 255,\n",
       " ('pasta', 0): 2,\n",
       " ('end', 0): 30,\n",
       " ('bbcmtd', 1): 1,\n",
       " ('wholesale', 1): 3,\n",
       " ('market', 1): 12,\n",
       " ('ablaze', 1): 12,\n",
       " ('httptcolhyxeohyc', 1): 1,\n",
       " ('always', 0): 36,\n",
       " ('try', 0): 18,\n",
       " ('bring', 0): 12,\n",
       " ('heavy', 0): 2,\n",
       " ('metal', 0): 8,\n",
       " ('rt', 0): 60,\n",
       " ('httptcoyaoexngw', 0): 1,\n",
       " ('africanbaze', 1): 1,\n",
       " ('newsnigeria', 1): 1,\n",
       " ('flag', 1): 19,\n",
       " ('set', 1): 26,\n",
       " ('aba', 1): 5,\n",
       " ('httptconndbgwyei', 1): 1,\n",
       " ('cry', 0): 16,\n",
       " ('set', 0): 26,\n",
       " ('ablaze', 0): 16,\n",
       " ('plus', 0): 3,\n",
       " ('side', 0): 19,\n",
       " ('look', 0): 72,\n",
       " ('sky', 0): 12,\n",
       " ('night', 0): 41,\n",
       " ('httptcoqqsmshajn', 0): 1,\n",
       " ('phdsquares', 0): 1,\n",
       " ('mufc', 0): 2,\n",
       " ('theyve', 0): 2,\n",
       " ('built', 0): 6,\n",
       " ('much', 0): 51,\n",
       " ('hype', 0): 3,\n",
       " ('around', 0): 23,\n",
       " ('new', 0): 168,\n",
       " ('acquisition', 0): 2,\n",
       " ('doubt', 0): 5,\n",
       " ('epl', 0): 1,\n",
       " ('season', 0): 10,\n",
       " ('inec', 1): 2,\n",
       " ('office', 1): 9,\n",
       " ('abia', 1): 2,\n",
       " ('httptcoimaomknna', 1): 1,\n",
       " ('barbados', 1): 1,\n",
       " ('bridgetown', 1): 1,\n",
       " ('jamaica', 1): 4,\n",
       " ('\\x89ûò', 1): 22,\n",
       " ('two', 1): 72,\n",
       " ('santa', 1): 4,\n",
       " ('cruz', 1): 4,\n",
       " ('\\x89ûó', 1): 10,\n",
       " ('head', 1): 11,\n",
       " ('st', 1): 27,\n",
       " ('elizabeth', 1): 1,\n",
       " ('police', 1): 107,\n",
       " ('superintende', 1): 1,\n",
       " ('httptcowdueajqj', 1): 1,\n",
       " ('lord', 0): 17,\n",
       " ('check', 0): 39,\n",
       " ('httptcoroinsmejj', 0): 2,\n",
       " ('httptcotjzjin', 0): 2,\n",
       " ('httptcoyduixefipe', 0): 2,\n",
       " ('httptcolxtjckls', 0): 2,\n",
       " ('nsfw', 0): 3,\n",
       " ('outside', 0): 13,\n",
       " ('youre', 0): 58,\n",
       " ('alive', 0): 7,\n",
       " ('dead', 0): 33,\n",
       " ('inside', 0): 13,\n",
       " ('awesome', 0): 15,\n",
       " ('time', 0): 105,\n",
       " ('visiting', 0): 1,\n",
       " ('cfc', 0): 2,\n",
       " ('head', 0): 37,\n",
       " ('office', 0): 5,\n",
       " ('ancop', 0): 1,\n",
       " ('site', 0): 7,\n",
       " ('thanks', 0): 22,\n",
       " ('tita', 0): 1,\n",
       " ('vida', 0): 2,\n",
       " ('taking', 0): 9,\n",
       " ('care', 0): 21,\n",
       " ('u', 0): 147,\n",
       " ('soooo', 0): 2,\n",
       " ('pumped', 0): 1,\n",
       " ('southridgelife', 0): 1,\n",
       " ('wanted', 0): 13,\n",
       " ('chicago', 0): 4,\n",
       " ('preaching', 0): 1,\n",
       " ('hotel', 0): 4,\n",
       " ('httptcooqknbfofx', 0): 1,\n",
       " ('gained', 0): 6,\n",
       " ('follower', 0): 8,\n",
       " ('know', 0): 90,\n",
       " ('stats', 0): 3,\n",
       " ('grow', 0): 6,\n",
       " ('httptcotiyulifc', 0): 1,\n",
       " ('west', 1): 22,\n",
       " ('burned', 1): 10,\n",
       " ('thousand', 1): 13,\n",
       " ('alone', 1): 5,\n",
       " ('httptcovltbrwbr', 1): 1,\n",
       " ('building', 0): 46,\n",
       " ('perfect', 0): 8,\n",
       " ('tracklist', 0): 1,\n",
       " ('life', 0): 67,\n",
       " ('leave', 0): 15,\n",
       " ('street', 0): 8,\n",
       " ('first', 0): 58,\n",
       " ('retainer', 0): 1,\n",
       " ('quite', 0): 7,\n",
       " ('weird', 0): 6,\n",
       " ('better', 0): 33,\n",
       " ('get', 0): 185,\n",
       " ('used', 0): 20,\n",
       " ('wear', 0): 4,\n",
       " ('every', 0): 44,\n",
       " ('single', 0): 9,\n",
       " ('next', 0): 33,\n",
       " ('year', 0): 64,\n",
       " ('least', 0): 10,\n",
       " ('deputy', 1): 6,\n",
       " ('man', 1): 48,\n",
       " ('shot', 1): 31,\n",
       " ('brighton', 1): 2,\n",
       " ('home', 1): 100,\n",
       " ('httptcogwnrhmsok', 1): 1,\n",
       " ('wife', 1): 5,\n",
       " ('get', 1): 70,\n",
       " ('six', 1): 4,\n",
       " ('year', 1): 86,\n",
       " ('jail', 1): 1,\n",
       " ('setting', 1): 7,\n",
       " ('niece', 1): 1,\n",
       " ('httptcoevahoucza', 1): 1,\n",
       " ('santa', 0): 2,\n",
       " ('cruz', 0): 6,\n",
       " ('\\x89ûó', 0): 16,\n",
       " ('st', 0): 19,\n",
       " ('elizabeth', 0): 1,\n",
       " ('police', 0): 33,\n",
       " ('superintendent', 0): 1,\n",
       " ('lanford', 0): 1,\n",
       " ('salmon', 0): 2,\n",
       " ('r', 0): 22,\n",
       " ('httptcovplrhkau', 0): 1,\n",
       " ('httptcosxhwtnnlf', 0): 1,\n",
       " ('arsonist', 1): 7,\n",
       " ('deliberately', 1): 1,\n",
       " ('black', 1): 20,\n",
       " ('church', 1): 5,\n",
       " ('north', 1): 22,\n",
       " ('carolinaåêablaze', 1): 1,\n",
       " ('httptcopcxarbhan', 1): 1,\n",
       " ('noches', 0): 1,\n",
       " ('elbestia', 0): 1,\n",
       " ('alexissanchez', 0): 1,\n",
       " ('happy', 0): 21,\n",
       " ('see', 0): 84,\n",
       " ('teammate', 0): 1,\n",
       " ('training', 0): 4,\n",
       " ('hard', 0): 13,\n",
       " ('goodnight', 0): 1,\n",
       " ('gunner', 0): 1,\n",
       " ('httptcoucjjhvgr', 0): 1,\n",
       " ('kurd', 1): 3,\n",
       " ('trampling', 1): 1,\n",
       " ('turkmen', 1): 2,\n",
       " ('later', 1): 7,\n",
       " ('others', 1): 11,\n",
       " ('vandalized', 1): 1,\n",
       " ('front', 1): 6,\n",
       " ('diyala', 1): 1,\n",
       " ('httptcoizfdyccg', 1): 1,\n",
       " ('truck', 1): 34,\n",
       " ('r', 1): 4,\n",
       " ('voortrekker', 1): 1,\n",
       " ('ave', 1): 11,\n",
       " ('outside', 1): 12,\n",
       " ('tambo', 1): 1,\n",
       " ('intl', 1): 1,\n",
       " ('cargo', 1): 1,\n",
       " ('section', 1): 1,\n",
       " ('httptcokscqkfkkf', 1): 1,\n",
       " ('heart', 0): 28,\n",
       " ('city', 0): 20,\n",
       " ('gift', 0): 3,\n",
       " ('skyline', 0): 2,\n",
       " ('kiss', 0): 3,\n",
       " ('upon', 0): 13,\n",
       " ('lip', 0): 5,\n",
       " ('\\x89û', 0): 30,\n",
       " ('httpstcocyompzaz', 0): 1,\n",
       " ('tonight', 0): 24,\n",
       " ('los', 0): 2,\n",
       " ('angeles', 0): 2,\n",
       " ('im', 0): 243,\n",
       " ('expecting', 0): 4,\n",
       " ('ig', 0): 3,\n",
       " ('fb', 0): 3,\n",
       " ('filled', 0): 3,\n",
       " ('sunset', 0): 3,\n",
       " ('shot', 0): 11,\n",
       " ('peep', 0): 2,\n",
       " ('httptcoicsjgzte', 1): 1,\n",
       " ('climate', 1): 11,\n",
       " ('energy', 1): 3,\n",
       " ('httptcofxmnlbd', 1): 1,\n",
       " ('revel', 0): 1,\n",
       " ('wmv', 0): 2,\n",
       " ('video', 0): 102,\n",
       " ('mean', 0): 20,\n",
       " ('mac', 0): 6,\n",
       " ('farewell', 0): 1,\n",
       " ('en', 0): 2,\n",
       " ('route', 0): 5,\n",
       " ('dvd', 0): 5,\n",
       " ('gtxrwm', 0): 1,\n",
       " ('progressive', 0): 2,\n",
       " ('greeting', 0): 1,\n",
       " ('month', 0): 14,\n",
       " ('student', 0): 7,\n",
       " ('would', 0): 97,\n",
       " ('pen', 0): 1,\n",
       " ('torch', 0): 3,\n",
       " ('publication', 0): 1,\n",
       " ('httptcofxpixqujt', 0): 1,\n",
       " ('rene', 0): 2,\n",
       " ('amp', 0): 193,\n",
       " ('jacinta', 0): 1,\n",
       " ('secret', 0): 21,\n",
       " ('k', 0): 16,\n",
       " ('fallen', 0): 1,\n",
       " ('edit', 0): 2,\n",
       " ('mar', 0): 1,\n",
       " ('httpstcomlmsuzvz', 0): 1,\n",
       " ('navista', 1): 1,\n",
       " ('steve', 1): 2,\n",
       " ('something', 1): 10,\n",
       " ('else', 1): 6,\n",
       " ('tinderbox', 1): 1,\n",
       " ('clown', 1): 1,\n",
       " ('hood', 1): 1,\n",
       " ('news', 1): 140,\n",
       " ('nowplaying', 0): 24,\n",
       " ('ian', 0): 2,\n",
       " ('buff', 0): 2,\n",
       " ('magnitude', 0): 1,\n",
       " ('httptcoavjsjfftc', 0): 1,\n",
       " ('edm', 0): 11,\n",
       " ('nxwestmidlands', 1): 1,\n",
       " ('huge', 1): 13,\n",
       " ('httptcorwzbfvnxer', 1): 1,\n",
       " ('talk', 0): 15,\n",
       " ('go', 0): 91,\n",
       " ('dont', 0): 141,\n",
       " ('make', 0): 75,\n",
       " ('due', 0): 7,\n",
       " ('work', 0): 57,\n",
       " ('kid', 0): 29,\n",
       " ('cuz', 0): 5,\n",
       " ('got', 0): 91,\n",
       " ('bicycle', 0): 2,\n",
       " ('accident', 0): 24,\n",
       " ('split', 0): 2,\n",
       " ('testicle', 0): 1,\n",
       " ('impossible', 0): 4,\n",
       " ('michael', 0): 10,\n",
       " ('father', 0): 5,\n",
       " ('accident', 1): 71,\n",
       " ('w', 1): 16,\n",
       " ('nashvilletraffic', 1): 1,\n",
       " ('traffic', 1): 22,\n",
       " ('moving', 1): 10,\n",
       " ('slower', 1): 2,\n",
       " ('usual', 1): 3,\n",
       " ('httpstcoghkegj', 1): 1,\n",
       " ('center', 1): 16,\n",
       " ('lane', 1): 11,\n",
       " ('blocked', 1): 5,\n",
       " ('santaclara', 1): 1,\n",
       " ('nb', 1): 5,\n",
       " ('great', 1): 15,\n",
       " ('america', 1): 11,\n",
       " ('pkwy', 1): 2,\n",
       " ('bayarea', 1): 1,\n",
       " ('httptcopmlohzurwr', 1): 1,\n",
       " ('httptcogkyegjtk', 0): 1,\n",
       " ('personalinjury', 0): 1,\n",
       " ('read', 0): 44,\n",
       " ('advice', 0): 2,\n",
       " ('solicitor', 0): 2,\n",
       " ('help', 0): 50,\n",
       " ('otleyhour', 0): 1,\n",
       " ('stlouis', 0): 1,\n",
       " ('caraccidentlawyer', 0): 1,\n",
       " ('speeding', 0): 1,\n",
       " ('among', 0): 6,\n",
       " ('top', 0): 41,\n",
       " ('cause', 0): 30,\n",
       " ('teen', 0): 11,\n",
       " ('httpstcokzomof', 0): 1,\n",
       " ('httpstcoskxvmcba', 0): 1,\n",
       " ('tee\\x89û', 0): 1,\n",
       " ('reported', 1): 14,\n",
       " ('motor', 1): 3,\n",
       " ('vehicle', 1): 16,\n",
       " ('curry', 1): 1,\n",
       " ('herman', 1): 1,\n",
       " ('rd', 1): 25,\n",
       " ('stephenson', 1): 1,\n",
       " ('involving', 1): 5,\n",
       " ('overturned', 1): 1,\n",
       " ('please', 1): 26,\n",
       " ('use', 1): 14,\n",
       " ('httptcoybjezkurw', 1): 1,\n",
       " ('bigrigradio', 1): 1,\n",
       " ('awareness', 1): 2,\n",
       " ('mile', 1): 10,\n",
       " ('marker', 1): 3,\n",
       " ('mooresville', 1): 2,\n",
       " ('iredell', 1): 2,\n",
       " ('ramp', 1): 1,\n",
       " ('pm', 1): 89,\n",
       " ('sleepjunkies', 0): 1,\n",
       " ('sleeping', 0): 9,\n",
       " ('pill', 0): 3,\n",
       " ('double', 0): 9,\n",
       " ('risk', 0): 8,\n",
       " ('httptcosnmfict', 0): 1,\n",
       " ('knew', 0): 6,\n",
       " ('gon', 0): 36,\n",
       " ('happen', 0): 6,\n",
       " ('httpstcoysxunvceh', 0): 1,\n",
       " ('n', 1): 18,\n",
       " ('cabrillo', 1): 1,\n",
       " ('hwymagellan', 1): 1,\n",
       " ('av', 1): 5,\n",
       " ('mir', 1): 1,\n",
       " ('congestion', 1): 1,\n",
       " ('pastor', 1): 1,\n",
       " ('scene', 1): 13,\n",
       " ('accidentwho', 1): 1,\n",
       " ('owner', 1): 6,\n",
       " ('range', 1): 2,\n",
       " ('rover', 1): 2,\n",
       " ('mom', 0): 19,\n",
       " ('didnt', 0): 21,\n",
       " ('home', 0): 38,\n",
       " ('wished', 0): 1,\n",
       " ('truck', 0): 20,\n",
       " ('spilt', 0): 1,\n",
       " ('mayonnaise', 0): 1,\n",
       " ('horrible', 1): 15,\n",
       " ('past', 1): 21,\n",
       " ('sunday', 1): 5,\n",
       " ('finally', 1): 2,\n",
       " ('able', 1): 3,\n",
       " ('around', 1): 16,\n",
       " ('thank', 1): 6,\n",
       " ('god', 1): 15,\n",
       " ('wait', 0): 22,\n",
       " ('pissed', 0): 2,\n",
       " ('donnie', 0): 1,\n",
       " ('tell', 0): 17,\n",
       " ('another', 0): 37,\n",
       " ('truckcrash', 1): 1,\n",
       " ('overturn', 1): 2,\n",
       " ('fortworth', 1): 1,\n",
       " ('interstate', 1): 2,\n",
       " ('httptcorsljqfp', 1): 1,\n",
       " ('click', 1): 2,\n",
       " ('youve', 1): 1,\n",
       " ('crashgthttptcolduniywk', 1): 1,\n",
       " ('ashville', 1): 1,\n",
       " ('sb', 1): 4,\n",
       " ('sr', 1): 4,\n",
       " ('httptcohylmowgfi', 1): 1,\n",
       " ('carolina', 1): 1,\n",
       " ('motorcyclist', 1): 9,\n",
       " ('dy', 1): 11,\n",
       " ('crossed', 1): 2,\n",
       " ('median', 1): 1,\n",
       " ('motorcycle', 1): 6,\n",
       " ('rider', 1): 5,\n",
       " ('traveling', 1): 1,\n",
       " ('httptcoplzrlmy', 1): 1,\n",
       " ('fyi', 1): 3,\n",
       " ('cadfyi', 1): 2,\n",
       " ('property', 1): 9,\n",
       " ('damagenhs', 1): 1,\n",
       " ('piner', 1): 2,\n",
       " ('rdhorndale', 1): 2,\n",
       " ('dr', 1): 7,\n",
       " ('rt', 1): 47,\n",
       " ('naayf', 1): 1,\n",
       " ('first', 1): 49,\n",
       " ('turning', 1): 1,\n",
       " ('onto', 1): 8,\n",
       " ('chandanee', 1): 1,\n",
       " ('magu', 1): 1,\n",
       " ('mma', 1): 5,\n",
       " ('taxi', 1): 2,\n",
       " ('rammed', 1): 2,\n",
       " ('halfway', 1): 1,\n",
       " ('turned', 1): 5,\n",
       " ('everyone', 1): 15,\n",
       " ('conf\\x89û', 1): 1,\n",
       " ('left', 1): 10,\n",
       " ('manchester', 1): 9,\n",
       " ('eddy', 1): 1,\n",
       " ('stop', 1): 20,\n",
       " ('go', 1): 35,\n",
       " ('back', 1): 36,\n",
       " ('nha', 1): 1,\n",
       " ('delay', 1): 7,\n",
       " ('min', 1): 3,\n",
       " ('httptcooiafxigm', 0): 1,\n",
       " ('damagewpd', 1): 1,\n",
       " ('th', 1): 38,\n",
       " ('injury', 1): 27,\n",
       " ('willis', 1): 1,\n",
       " ('foreman', 1): 1,\n",
       " ('httptcovckitedev', 1): 1,\n",
       " ('aashiqui', 1): 1,\n",
       " ('actress', 1): 2,\n",
       " ('anu', 1): 1,\n",
       " ('aggarwal', 1): 1,\n",
       " ('nearfatal', 1): 1,\n",
       " ('httptcootfplqw', 1): 1,\n",
       " ('suffield', 1): 1,\n",
       " ('alberta', 1): 5,\n",
       " ('httpstcobptmlfp', 1): 1,\n",
       " ('backup', 1): 2,\n",
       " ('southaccident', 1): 1,\n",
       " ('blocking', 1): 4,\n",
       " ('right', 1): 23,\n",
       " ('exit', 1): 6,\n",
       " ('langtree', 1): 1,\n",
       " ('rdconsider', 1): 1,\n",
       " ('nc', 1): 9,\n",
       " ('alternate', 1): 1,\n",
       " ('changed', 0): 3,\n",
       " ('determine', 0): 1,\n",
       " ('option', 0): 7,\n",
       " ('financially', 0): 1,\n",
       " ('support', 0): 14,\n",
       " ('plan', 0): 40,\n",
       " ('ongoing', 0): 1,\n",
       " ('treatment', 0): 5,\n",
       " ('deadly', 1): 7,\n",
       " ('happened', 1): 10,\n",
       " ('hagerstown', 1): 1,\n",
       " ('today', 1): 53,\n",
       " ('ill', 1): 9,\n",
       " ('detail', 1): 4,\n",
       " ('yourstate', 1): 1,\n",
       " ('whag', 1): 1,\n",
       " ('flowri', 0): 1,\n",
       " ('marinading', 0): 1,\n",
       " ('even', 1): 21,\n",
       " ('week', 1): 11,\n",
       " ('fucking', 1): 6,\n",
       " ('mf', 1): 2,\n",
       " ('cant', 1): 21,\n",
       " ('drive', 1): 11,\n",
       " ('norwaymfa', 1): 1,\n",
       " ('bahrain', 1): 1,\n",
       " ('previously', 1): 4,\n",
       " ('road', 1): 28,\n",
       " ('killed', 1): 93,\n",
       " ('explosion', 1): 28,\n",
       " ('httpstcogfjfgtodad', 1): 1,\n",
       " ('still', 0): 72,\n",
       " ('heard', 0): 24,\n",
       " ('church', 0): 4,\n",
       " ('leader', 0): 4,\n",
       " ('kenya', 0): 3,\n",
       " ('coming', 0): 36,\n",
       " ('forward', 0): 4,\n",
       " ('comment', 0): 13,\n",
       " ('issue', 0): 13,\n",
       " ('disciplinary', 0): 1,\n",
       " ('measuresarrestpastornganga', 0): 1,\n",
       " ('aftershockdelo', 0): 2,\n",
       " ('scuf', 0): 2,\n",
       " ('p', 0): 18,\n",
       " ('live', 0): 30,\n",
       " ('game', 0): 40,\n",
       " ('cya', 0): 1,\n",
       " ('drive', 0): 10,\n",
       " ('effort', 0): 6,\n",
       " ('painful', 0): 1,\n",
       " ('win', 0): 21,\n",
       " ('roger', 0): 2,\n",
       " ('bannister', 0): 1,\n",
       " ('ir', 0): 7,\n",
       " ('icemoon', 0): 7,\n",
       " ('aftershock', 0): 19,\n",
       " ('httptcoynxnvvkcda', 0): 1,\n",
       " ('djicemoon', 0): 7,\n",
       " ('dubstep', 0): 7,\n",
       " ('trapmusic', 0): 7,\n",
       " ('dnb', 0): 8,\n",
       " ('dance', 0): 15,\n",
       " ('ices\\x89û', 0): 7,\n",
       " ('httptcoweqpesenku', 0): 1,\n",
       " ('victory', 0): 3,\n",
       " ('bargain', 0): 4,\n",
       " ('basement', 0): 2,\n",
       " ('price', 0): 7,\n",
       " ('dwight', 0): 1,\n",
       " ('david', 0): 6,\n",
       " ('eisenhower', 0): 1,\n",
       " ('httptcovampodgyw', 0): 2,\n",
       " ('httptcozevakjapcz', 0): 2,\n",
       " ('nobody', 0): 3,\n",
       " ('remembers', 0): 1,\n",
       " ('came', 0): 21,\n",
       " ('second', 0): 17,\n",
       " ('charles', 0): 2,\n",
       " ('schulz', 0): 1,\n",
       " ('speaking', 0): 2,\n",
       " ('someone', 0): 34,\n",
       " ('using', 0): 5,\n",
       " ('xb', 0): 2,\n",
       " ('people', 0): 93,\n",
       " ('getting', 0): 38,\n",
       " ('also', 0): 33,\n",
       " ('harder', 0): 1,\n",
       " ('conflict', 0): 2,\n",
       " ('glorious', 0): 2,\n",
       " ('triumph', 0): 2,\n",
       " ('thomas', 0): 6,\n",
       " ('paine', 0): 1,\n",
       " ('growingupspoiled', 0): 1,\n",
       " ('going', 0): 75,\n",
       " ('clay', 0): 1,\n",
       " ('pigeon', 0): 1,\n",
       " ('shooting', 0): 6,\n",
       " ('guess', 0): 11,\n",
       " ('one', 0): 136,\n",
       " ('actually', 0): 18,\n",
       " ('want', 0): 81,\n",
       " ('free', 0): 32,\n",
       " ('tc', 0): 2,\n",
       " ('terrifying', 0): 1,\n",
       " ('best', 0): 54,\n",
       " ('roller', 0): 3,\n",
       " ('coaster', 0): 2,\n",
       " ('ive', 0): 36,\n",
       " ('ever', 0): 41,\n",
       " ('disclaimer', 0): 1,\n",
       " ('httpstcoxmwodfmtui', 0): 1,\n",
       " ('httptcomjdzmgjow', 0): 1,\n",
       " ('httptconuhasfkbv', 0): 1,\n",
       " ('httptcoeepzhoth', 0): 1,\n",
       " ('httptcoaddoq', 0): 1,\n",
       " ('kjfordays', 0): 1,\n",
       " ('seeing', 0): 14,\n",
       " ('httptcothyzomvwu', 0): 2,\n",
       " ('httptcojooxk', 0): 2,\n",
       " ('wisdomwed', 0): 1,\n",
       " ('bonus', 0): 1,\n",
       " ('minute', 0): 12,\n",
       " ('daily', 0): 11,\n",
       " ('habit', 0): 3,\n",
       " ('could', 0): 43,\n",
       " ('really', 0): 55,\n",
       " ('improve', 0): 1,\n",
       " ('many', 0): 51,\n",
       " ('already', 0): 22,\n",
       " ('lifehacks', 0): 1,\n",
       " ('httptcotbmfqbcw', 0): 1,\n",
       " ('protect', 0): 4,\n",
       " ('profit', 0): 3,\n",
       " ('global', 0): 4,\n",
       " ('financial', 0): 9,\n",
       " ('meltdown', 0): 26,\n",
       " ('wiedemer', 0): 1,\n",
       " ('http', 0): 1,\n",
       " ('httptcowztzhgmvq', 0): 1,\n",
       " ('moment', 0): 15,\n",
       " ('scary', 0): 4,\n",
       " ('guy', 0): 37,\n",
       " ('behind', 0): 10,\n",
       " ('screaming', 0): 37,\n",
       " ('bloody', 0): 37,\n",
       " ('murder', 0): 9,\n",
       " ('silverwood', 0): 1,\n",
       " ('\\x89ã¢', 0): 1,\n",
       " ('full\\x89ã¢', 0): 1,\n",
       " ('streaming', 0): 2,\n",
       " ('youtube', 0): 76,\n",
       " ('httptcovveusesgf', 0): 1,\n",
       " ('gtgt', 0): 12,\n",
       " ('book', 0): 29,\n",
       " ('httptcofntucz', 0): 1,\n",
       " ('esquireattire', 0): 1,\n",
       " ('sometimes', 0): 11,\n",
       " ('face', 0): 35,\n",
       " ('difficulty', 0): 2,\n",
       " ('something', 0): 22,\n",
       " ('wrong', 0): 19,\n",
       " ('right', 0): 50,\n",
       " ('joel', 0): 3,\n",
       " ('osteen', 0): 1,\n",
       " ('thing', 0): 53,\n",
       " ('stand', 0): 20,\n",
       " ('dream', 0): 12,\n",
       " ('belief', 0): 2,\n",
       " ('possible', 0): 8,\n",
       " ('brown', 0): 20,\n",
       " ('praise', 0): 1,\n",
       " ('god', 0): 49,\n",
       " ('ministry', 0): 1,\n",
       " ('wdyouth', 0): 1,\n",
       " ('biblestudy', 0): 1,\n",
       " ('httpstcoujkegbcc', 0): 1,\n",
       " ('remembering', 0): 3,\n",
       " ('die', 0): 16,\n",
       " ('way', 0): 59,\n",
       " ('avoid', 0): 4,\n",
       " ('trap', 0): 4,\n",
       " ('thinking', 0): 13,\n",
       " ('lose', 0): 8,\n",
       " ('\\x89ûò', 0): 18,\n",
       " ('steve', 0): 6,\n",
       " ('job', 0): 40,\n",
       " ('tried', 0): 12,\n",
       " ('orange', 0): 2,\n",
       " ('today', 0): 46,\n",
       " ('never', 0): 44,\n",
       " ('onfireanders', 0): 1,\n",
       " ('bb', 0): 7,\n",
       " ('httpstcojvppkhjy', 0): 1,\n",
       " ('back', 0): 85,\n",
       " ('school', 0): 38,\n",
       " ('kick', 0): 8,\n",
       " ('great', 0): 47,\n",
       " ('thank', 0): 26,\n",
       " ('everyone', 0): 38,\n",
       " ('making', 0): 22,\n",
       " ('say', 0): 63,\n",
       " ('done', 0): 17,\n",
       " ('interrupt', 0): 1,\n",
       " ('george', 0): 4,\n",
       " ('bernard', 0): 1,\n",
       " ('shaw', 0): 2,\n",
       " ('oyster', 0): 1,\n",
       " ('shell', 0): 3,\n",
       " ('andrew', 0): 2,\n",
       " ('carnegie', 0): 1,\n",
       " ('anyone', 0): 17,\n",
       " ('need', 0): 70,\n",
       " ('pu', 0): 1,\n",
       " ('play', 0): 27,\n",
       " ('hybrid', 0): 2,\n",
       " ('slayer', 0): 3,\n",
       " ('eu', 0): 2,\n",
       " ('hmu', 0): 2,\n",
       " ('codsandscrims', 0): 1,\n",
       " ('empirikgaming', 0): 1,\n",
       " ('codawscrims', 0): 1,\n",
       " ('tpkotc', 0): 1,\n",
       " ('tpfa', 0): 1,\n",
       " ('aftershockorg', 0): 1,\n",
       " ('expert', 1): 21,\n",
       " ('france', 1): 12,\n",
       " ('begin', 1): 12,\n",
       " ('examining', 1): 10,\n",
       " ('airplane', 1): 31,\n",
       " ('debris', 1): 50,\n",
       " ('found', 1): 44,\n",
       " ('reunion', 1): 32,\n",
       " ('island', 1): 36,\n",
       " ('french', 1): 9,\n",
       " ('air', 1): 32,\n",
       " ('httptcoyvvpznzmxg', 1): 1,\n",
       " ('strict', 1): 2,\n",
       " ('liability', 1): 2,\n",
       " ('context', 1): 2,\n",
       " ('pilot', 1): 13,\n",
       " ('error', 1): 2,\n",
       " ('common', 1): 2,\n",
       " ('component', 1): 1,\n",
       " ('aviation', 1): 2,\n",
       " ('cr', 1): 2,\n",
       " ('httptcoczbohrd', 1): 1,\n",
       " ('crobscarla', 0): 1,\n",
       " ('lifetime', 0): 3,\n",
       " ('odds', 0): 2,\n",
       " ('dying', 0): 6,\n",
       " ('airplane', 0): 5,\n",
       " ('wedn', 1): 2,\n",
       " ('httptcobkpfpogysi', 1): 1,\n",
       " ('alexalltimelow', 1): 1,\n",
       " ('awwww', 1): 1,\n",
       " ('theyre', 1): 8,\n",
       " ('die', 1): 8,\n",
       " ('cuties', 1): 1,\n",
       " ('good', 1): 20,\n",
       " ('job', 1): 6,\n",
       " ('family', 1): 105,\n",
       " ('member', 1): 11,\n",
       " ('osama', 1): 1,\n",
       " ('bin', 1): 9,\n",
       " ('laden', 1): 8,\n",
       " ('ironic', 1): 2,\n",
       " ('mhmmm', 1): 1,\n",
       " ('gov', 1): 5,\n",
       " ('shit', 1): 19,\n",
       " ('suspect', 1): 36,\n",
       " ('engine', 1): 4,\n",
       " ('httptcotyjxrfdst', 1): 1,\n",
       " ('via', 1): 121,\n",
       " ('youtube', 1): 22,\n",
       " ('wing', 1): 8,\n",
       " ('httptcoikztevbv', 1): 1,\n",
       " ('cessna', 1): 1,\n",
       " ('ocampo', 1): 1,\n",
       " ('coahuila', 1): 2,\n",
       " ('mexico', 1): 6,\n",
       " ('july', 1): 4,\n",
       " ('four', 1): 10,\n",
       " ('men', 1): 13,\n",
       " ('including', 1): 6,\n",
       " ('state', 1): 40,\n",
       " ('government', 1): 20,\n",
       " ('official', 1): 37,\n",
       " ('watchthevideo', 1): 1,\n",
       " ('httptcopxrvgjik', 1): 1,\n",
       " ('httptcolsmxvwrj', 1): 1,\n",
       " ('wednesday\\x89û', 1): 1,\n",
       " ('wednesday', 1): 11,\n",
       " ('began', 1): 5,\n",
       " ('kca', 1): 3,\n",
       " ('votejktid', 1): 3,\n",
       " ('mbataweel', 1): 1,\n",
       " ('rip', 1): 4,\n",
       " ('binladen', 1): 1,\n",
       " ('almost', 0): 14,\n",
       " ('sent', 0): 9,\n",
       " ('coworker', 0): 1,\n",
       " ('nude', 0): 3,\n",
       " ('mode', 0): 15,\n",
       " ('mickinyman', 0): 1,\n",
       " ('theatlantic', 0): 1,\n",
       " ('might', 0): 17,\n",
       " ('killed', 0): 3,\n",
       " ('wreck', 0): 47,\n",
       " ('politics', 0): 2,\n",
       " ('httptcotagzbcxfj', 1): 1,\n",
       " ('mlb', 1): 3,\n",
       " ('unbelievably', 1): 1,\n",
       " ('insane', 1): 4,\n",
       " ('airport', 1): 30,\n",
       " ('aircraft', 1): 24,\n",
       " ('aeroplane', 1): 1,\n",
       " ('runway', 1): 7,\n",
       " ('freaky\\x89û', 1): 1,\n",
       " ('httpstcocezhqczll', 1): 1,\n",
       " ('airplaneåê', 1): 1,\n",
       " ('httptcowqwjsgphl', 1): 1,\n",
       " ('httptcotfcdronra', 1): 1,\n",
       " ('usama', 1): 1,\n",
       " ('ladin', 1): 1,\n",
       " ('dead', 1): 63,\n",
       " ('naturally', 1): 1,\n",
       " ('plane', 1): 31,\n",
       " ('festival', 1): 3,\n",
       " ('httpstcokqaeapb', 1): 1,\n",
       " ('death', 1): 65,\n",
       " ('carfest', 1): 1,\n",
       " ('httptcogibyqhhkpk', 1): 1,\n",
       " ('dtn', 1): 4,\n",
       " ('brazil', 1): 6,\n",
       " ('exp', 1): 2,\n",
       " ('httptcomigwqlq', 1): 1,\n",
       " ('httptcovsmaeslk', 1): 1,\n",
       " ('\\x89ûïairplane\\x89û\\x9d', 1): 1,\n",
       " ('wtf', 1): 5,\n",
       " ('can\\x89ûªt', 1): 1,\n",
       " ('believe', 1): 11,\n",
       " ('eye', 1): 9,\n",
       " ('httptcoffylajwps', 1): 1,\n",
       " ('nicole', 1): 1,\n",
       " ('fletcher', 1): 1,\n",
       " ('one', 1): 69,\n",
       " ('victim', 1): 24,\n",
       " ('crashed', 1): 22,\n",
       " ('time', 1): 69,\n",
       " ('ago', 1): 16,\n",
       " ('little', 1): 17,\n",
       " ('bit', 1): 7,\n",
       " ('trauma', 1): 19,\n",
       " ('although', 1): 1,\n",
       " ('shes', 1): 2,\n",
       " ('omg', 1): 10,\n",
       " ('httptcoxdxdprcpns', 1): 1,\n",
       " ('dont', 1): 50,\n",
       " ('bro', 1): 2,\n",
       " ('jetengine', 1): 1,\n",
       " ('turbojet', 1): 1,\n",
       " ('boing', 1): 1,\n",
       " ('g', 1): 6,\n",
       " ('httptcokxxnszpnk', 1): 1,\n",
       " ('phone', 0): 32,\n",
       " ('ship', 0): 22,\n",
       " ('terrible', 0): 4,\n",
       " ('statistically', 0): 1,\n",
       " ('cop', 0): 6,\n",
       " ('house', 1): 38,\n",
       " ('colombia', 1): 1,\n",
       " ('httpstcozhjlflbhzl', 1): 1,\n",
       " ('shooting', 1): 24,\n",
       " ('httpstcoieccjdoub', 1): 1,\n",
       " ('could', 1): 38,\n",
       " ('drone', 1): 9,\n",
       " ('worried', 1): 4,\n",
       " ('esp', 1): 2,\n",
       " ('close', 1): 11,\n",
       " ('vicinity', 1): 2,\n",
       " ('httptcokzrgngjf', 1): 1,\n",
       " ('early', 1): 12,\n",
       " ('wake', 1): 16,\n",
       " ('call', 1): 24,\n",
       " ('sister', 1): 4,\n",
       " ('begging', 1): 1,\n",
       " ('come', 1): 21,\n",
       " ('amp', 1): 107,\n",
       " ('ride', 1): 1,\n",
       " ('wher', 1): 1,\n",
       " ('ambulance', 1): 24,\n",
       " ('hospital', 1): 7,\n",
       " ('rodkiai', 1): 1,\n",
       " ('httptcoayzzcupnz', 1): 1,\n",
       " ('twelve', 1): 14,\n",
       " ('feared', 1): 22,\n",
       " ('pakistani', 1): 16,\n",
       " ('helicopter', 1): 26,\n",
       " ('httptcoscdnsmc', 1): 1,\n",
       " ('serious', 1): 11,\n",
       " ('lorry', 1): 5,\n",
       " ('httptcopfeaqeski', 1): 1,\n",
       " ('httptcofntgrnkx', 1): 1,\n",
       " ('emsne\\x89û', 1): 1,\n",
       " ('reuters', 1): 23,\n",
       " ('httptcomdnugvubwn', 1): 1,\n",
       " ('yugvani', 1): 3,\n",
       " ('leading', 0): 4,\n",
       " ('emergency', 0): 82,\n",
       " ('service', 0): 41,\n",
       " ('bos', 0): 4,\n",
       " ('welcome', 0): 10,\n",
       " ('ambulance', 0): 17,\n",
       " ('charity', 0): 6,\n",
       " ('httptcomjjqpsv', 0): 1,\n",
       " ('anyone', 1): 7,\n",
       " ('travelling', 1): 2,\n",
       " ('aberystwythshrewsbury', 1): 1,\n",
       " ('incident', 1): 11,\n",
       " ('service', 1): 41,\n",
       " ('halt', 1): 2,\n",
       " ('shrew', 1): 1,\n",
       " ('httptcoxumylcbq', 1): 1,\n",
       " ('sprinter', 0): 4,\n",
       " ('automatic', 0): 5,\n",
       " ('frontline', 0): 4,\n",
       " ('vehicle', 0): 8,\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06be68b2",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), implement a naive bayes classifier.\n",
    "\n",
    "##### Calculate $V$\n",
    "- You can then compute the number of unique words that appear in the `freqs` dictionary to get your $V$ (you can use the `set` function).\n",
    "\n",
    "##### Calculate $freq_{pos}$ and $freq_{neg}$\n",
    "- Using your `freqs` dictionary, you can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n",
    "\n",
    "##### Calculate $N_{pos}$, and $N_{neg}$\n",
    "- Using `freqs` dictionary, you can also compute the total number of positive words and total number of negative words $N_{pos}$ and $N_{neg}$.\n",
    "\n",
    "##### Calculate $D$, $D_{pos}$, $D_{neg}$\n",
    "- Using the `train_y` input list of labels, calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n",
    "- Calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$\n",
    "\n",
    "##### Calculate the logprior\n",
    "- the logprior is $log(D_{pos}) - log(D_{neg})$\n",
    "\n",
    "##### Calculate log likelihood\n",
    "- Finally, you can iterate over each word in the vocabulary, use your `lookup` function to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n",
    "- Compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using equations 4 & 5.\n",
    "\n",
    "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
    "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
    "\n",
    "**Note:** We'll use a dictionary to store the log likelihoods for each word.  The key is the word, the value is the log likelihood of that word).\n",
    "\n",
    "- You can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e41124ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs , word , label):\n",
    "    tup = (word , label)\n",
    "    for key , val in freqs.items():\n",
    "        if key == tup:\n",
    "            return val\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98e6af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 GRADED FUNCTION: train_naive_bayes\n",
    "\n",
    "import math\n",
    "\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    '''\n",
    "    Input:\n",
    "        freqs: dictionary from (word, label) to how often the word appears\n",
    "        train_x: a list of tweets\n",
    "        train_y: a list of labels correponding to the tweets (0,1)\n",
    "    Output:\n",
    "        logprior: the log prior. (equation 3 above)\n",
    "        loglikelihood: the log likelihood of you Naive bayes equation. (equation 6 above)\n",
    "    '''\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)    \n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = V_pos = V_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "            # increment the count of unique positive words by 1\n",
    "            V_pos += 1\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "            # increment the count of unique negative words by 1\n",
    "            V_neg += 1\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "    \n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = 0\n",
    "    for i in range(len(train_y)):\n",
    "        if train_y[i] > 0:\n",
    "            D_pos += 1\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = 0\n",
    "    for i in range(len(train_y)):\n",
    "        if train_y[i] <= 0:\n",
    "            D_neg += 1\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = math.log(D_pos) - math.log(D_neg)\n",
    "    \n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff40dd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.28323932289985443\n",
      "20399\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, new_X , new_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ac45c",
   "metadata": {},
   "source": [
    "# Part 3: Test your naive bayes\n",
    "\n",
    "Now that we have the `logprior` and `loglikelihood`, we can test the naive bayes function by making predicting on some tweets!\n",
    "\n",
    "#### Implement `naive_bayes_predict`\n",
    "**Instructions**:\n",
    "Implement the `naive_bayes_predict` function to make predictions on tweets.\n",
    "* The function takes in the `tweet`, `logprior`, `loglikelihood`.\n",
    "* It returns the probability that the tweet belongs to the positive or negative class.\n",
    "* For each tweet, sum up loglikelihoods of each word in the tweet.\n",
    "* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n",
    "\n",
    "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
    "\n",
    "#### Note\n",
    "Note we calculate the prior from the training data, and that the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets).  This means that the ratio of positive to negative 1, and the logprior is 0.\n",
    "\n",
    "The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.  However, please remember to include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcbca19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 GRADED FUNCTION: naive_bayes_predict\n",
    "\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    '''\n",
    "    Input:\n",
    "        tweet: a string\n",
    "        logprior: a number\n",
    "        loglikelihood: a dictionary of words mapping to numbers\n",
    "    Output:\n",
    "        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n",
    "\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # process the tweet to get a list of words\n",
    "    word_l = preProcessing(tweet)\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "        \n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood.get(word)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "661ed52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected output is -0.28323932289985443\n"
     ]
    }
   ],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# Experiment with your own tweet.\n",
    "my_tweet = 'She smiled.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('The expected output is', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780611cc",
   "metadata": {},
   "source": [
    "#### Implement test_naive_bayes\n",
    "**Instructions**:\n",
    "* Implement `test_naive_bayes` to check the accuracy of your predictions.\n",
    "* The function takes in your `test_x`, `test_y`, log_prior, and loglikelihood\n",
    "* It returns the accuracy of your model.\n",
    "* First, use `naive_bayes_predict` function to make predictions for each tweet in text_x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "389a375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 GRADED FUNCTION: test_naive_bayes\n",
    "\n",
    "def test_naive_bayes(test_x , ids , test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        test_x: A list of tweets\n",
    "        test_y: the corresponding labels for the list of tweets\n",
    "        logprior: the logprior\n",
    "        loglikelihood: a dictionary with the loglikelihoods for each word\n",
    "    Output:\n",
    "        accuracy: (# of tweets classified correctly)/(total # of tweets)\n",
    "    \"\"\"\n",
    "    accuracy = 0  # return this properly\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    new_y_hats = []\n",
    "    \n",
    "    return_y_hats = np.zeros((len(ids) , 2))\n",
    "    \n",
    "    #counter = 0\n",
    "    for i in range(len(y_hats)):\n",
    "        new_y_hats.append(y_hats[i])\n",
    "            \n",
    "    \"\"\"#print(len(new_y_hats))\"\"\"\n",
    "    for i in range(len(new_y_hats)):\n",
    "        #print(ids[i])\n",
    "        return_y_hats[i][0] = ids[i]\n",
    "        return_y_hats[i][1] = new_y_hats[i]\n",
    "        \n",
    "    error = 0\n",
    "    result = []\n",
    "    for i in range(len(new_y_hats)):\n",
    "        result.append(np.absolute(new_y_hats[i]-test_y[i]))\n",
    "        \n",
    "    getSum = 0\n",
    "    for i in range(len(new_y_hats)):\n",
    "        getSum += result[i]\n",
    "        \n",
    "    error = getSum/len(result)\n",
    "    \n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1 - error \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return accuracy ,  new_y_hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfb15c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Process the test data \"\"\"\n",
    "new_test_set = convert_2d_array_to_list(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e9c60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d82388e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f396f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_array = sample.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad03bc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_array[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f08d1657",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for i in range(len(sample_array)):\n",
    "    ids.append(sample_array[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d211e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = []\n",
    "for i in range(len(sample_array)):\n",
    "    new_sample.append(sample_array[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b8bac40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0591f4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.6258\n"
     ]
    }
   ],
   "source": [
    "accuracy , y_hats = (test_naive_bayes(new_test_set, ids , new_sample, logprior, loglikelihood))\n",
    "\n",
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "599265cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros((len(y_hats) , 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "02efb4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b437378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(result)):\n",
    "    result[i][0] = ids[i]\n",
    "    result[i][1] = y_hats[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1fde2df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000e+00, 1.0000e+00],\n",
       "       [2.0000e+00, 1.0000e+00],\n",
       "       [3.0000e+00, 1.0000e+00],\n",
       "       ...,\n",
       "       [1.0868e+04, 1.0000e+00],\n",
       "       [1.0874e+04, 1.0000e+00],\n",
       "       [1.0875e+04, 1.0000e+00]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3d860f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hats_array = np.array(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8516a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_hats = y_hats_array.reshape((3263 , 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a02c8d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_hats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c55e3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result).to_csv('submission.csv',  header  = ['id' , 'target'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "811c3ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  target\n",
      "0         0       1\n",
      "1         2       1\n",
      "2         3       1\n",
      "3         9       1\n",
      "4        11       1\n",
      "...     ...     ...\n",
      "3258  10861       1\n",
      "3259  10865       1\n",
      "3260  10868       1\n",
      "3261  10874       1\n",
      "3262  10875       1\n",
      "\n",
      "[3263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pd.read_csv('submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b23794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
